# Ultimate Prompt Engineer v3.2

**Author:** Eduard Gubaidullin

## Overview

Welcome to **Ultimate Prompt Engineer v3.2**, a highly sophisticated meta-prompt designed to operate *within* Large Language Models (LLMs). Its primary purpose is to empower an LLM to act as an expert prompt engineer, capable of generating state-of-the-art (SOTA), highly optimized prompts based on user requirements.

This meta-prompt is intended for prompt engineers, AI researchers, and developers who need to generate high-quality, effective prompts for various LLMs and demanding tasks. It embodies a rigorous engineering process, focusing on clarity, effectiveness, model-specific adaptation, and continuous improvement.

## Key Features & Philosophy

Ultimate Prompt Engineer v3.2 distinguishes itself through several core principles and capabilities:

*   **State-of-the-Art Focus:** Mandated to leverage the absolute cutting edge of validated prompt engineering science, continuously integrating new research findings and methodologies.
*   **Deep Model Specialization & Cross-Model Versatility:** Possesses deep, constantly updated knowledge of major LLM families (OpenAI GPT series, Anthropic Claude series, Google Gemini series, Meta Llama series, xAI Grok, Mistral AI models, DeepSeek models, etc.), including their architectural nuances, capabilities, and limitations. While versatile, it excels when the target model is specified.
*   **Rigorous Engineering Process:** Follows a mandatory, structured `CORE PROCESS` to ensure comprehensive analysis, strategic design, and meticulous execution for every generated prompt.
*   **Built-in Quality Assurance:** Incorporates a non-negotiable, detailed `Rigorous Quality Assurance (QA)` checklist to guarantee prompts meet a 10+/10 quality standard, covering clarity, technique appropriateness, model optimization, and alignment with objectives.
*   **Advanced Technique Arsenal:** Masters and strategically applies a wide range of advanced prompting techniques (CoT, ReAct, Step-back, Few-shot, Persona Engineering, Structured Output, Self-Correction, etc.), selecting the optimal approach based on the task and target LLM.
*   **Clear Persona & Objective:** Operates under the defined persona of "Prompt Architect v3.2" with the ultimate objective of setting the pinnacle standard in LLM prompt engineering for internal use, synthesizing clarity, power, efficiency, and adaptability.

## How It Works: The Core Process

The meta-prompt guides the host LLM through a mandatory, step-by-step sequence to generate prompts:

1.  **üîç Comprehensive Task Analysis:** Deeply analyzes the user's request, identifying explicit/implicit needs, target LLM characteristics, constraints, and success metrics. Crucially, it asks clarifying questions to resolve ambiguities before proceeding.
2.  **üß© Strategic Technique Selection:** Chooses the most effective prompting architecture and techniques based on the task requirements and the specific capabilities and limitations of the target LLM.
3.  **üèóÔ∏è Precision Prompt Engineering:** Constructs the prompt with crystal-clear instructions, effective persona/role definition, model-optimized formatting (e.g., XML tags, Markdown), high-quality examples (if needed), and strategic use of constraints.
4.  **üî¨ Rigorous Quality Assurance (10+/10 Check):** Systematically evaluates the drafted prompt against an internal checklist covering clarity, technique suitability, model optimization, output control, consistency, and goal alignment. Iterates until the prompt meets the highest quality standard.
5.  **‚öôÔ∏è Configuration Optimization:** Recommends optimal runtime parameters (Temperature, Max Tokens, Stop Sequences, etc.) tailored to the generated prompt and the target LLM, balancing creativity and determinism.
6.  **üì¶ Final Delivery Structure:** Presents the finalized prompt(s) using clear Markdown formatting, outlining structure (e.g., ROLE, TASK, OUTPUT) and execution flow if multiple prompts are involved.
7.  **üìò Strategy Documentation & Guidance:** Provides concise explanations for the chosen strategies and techniques. Includes recommendations for evaluating the generated prompt (metrics, qualitative assessment, A/B testing) and advocates for clear versioning and documentation.
8.  **üîÑ Iteration Protocol:** Explicitly offers refinement and provides guidance on how to interpret model outputs for further prompt tuning and improvement using systematic testing.

## Meta-Prompt Structure Breakdown

The meta-prompt itself is organized into key sections that define its operation:

*   **‚≠ê PERSONA & MISSION:** Establishes the AI's identity as an expert prompt architect and its core purpose.
*   **üìö KNOWLEDGE & EXPERTISE:** Outlines the vast knowledge base required, covering prompt engineering principles, deep LLM specializations, and advanced techniques.
*   **üß† CORE PROCESS (Mandatory Execution Sequence):** Details the non-negotiable steps the AI must follow to generate a prompt.
*   **üî¨ Rigorous Quality Assurance (10+/10 Check):** Contains the specific checklist used for self-critique and quality validation.
*   **‚öôÔ∏è Configuration Optimization:** Guides the recommendation of runtime parameters.
*   **üìå CORE MANDATES:** Lists the fundamental rules and principles governing the AI's behavior (prioritize clarity, calibrate to LLM, use SOTA, exhaustive QA, etc.).
*   **üéØ ULTIMATE OBJECTIVE:** Defines the overarching goal of achieving the highest standard in prompt engineering.

## Target LLM Specification: Why It Matters

This meta-prompt places significant emphasis on identifying the target LLM (family, architecture, or specific model) for several critical reasons:

*   **Optimized Formatting:** Different LLMs respond best to different formatting conventions (e.g., specific use of System Prompts, XML tags, Markdown structure, JSON modes). Specifying the target allows the meta-prompt to use the most effective format.
*   **Capability Alignment:** Techniques are selected based on the *known and validated* capabilities of the target model (e.g., reasoning depth, context window size and behavior, function calling, handling specific instructions).
*   **Architectural Nuances:** Understanding underlying architectures (like Mixture-of-Experts) can inform prompt design for better performance.
*   **Performance Tuning:** Knowledge of a model's typical behavior allows for better recommendations regarding temperature, token limits, and other runtime parameters.

While the meta-prompt aims for adaptability, providing the target LLM details enables it to move beyond general best practices and apply highly specific, optimized strategies, maximizing the resulting prompt's effectiveness.

## Usage Guidelines

To leverage the full power of Ultimate Prompt Engineer v3.2 when instructing an LLM that is using this meta-prompt:

1.  **Provide Clear Requirements:** Be as specific and detailed as possible about the desired prompt's goal, intended input/output, target audience, tone, format, and any constraints.
2.  **Specify Target LLM:** Clearly state the intended target LLM (e.g., "Claude 3 Opus," "GPT-4 Turbo," "Gemini 1.5 Pro," "Llama 3 70B"). If unsure, specify the family or known characteristics. This is crucial for optimization.
3.  **Be Prepared for Clarification:** The meta-prompt is designed to ask clarifying questions if requirements are ambiguous. Be ready to provide additional details.
4.  **Understand the Output:** The result will be the engineered prompt itself, often accompanied by configuration recommendations and strategy explanations.
5.  **Iterate and Refine:** Use the provided evaluation guidance. If the generated prompt needs adjustments, provide specific feedback for refinement, leveraging the meta-prompt's iterative capabilities.

## Disclaimer

This meta-prompt is a sophisticated tool designed for advanced users. The quality of the generated prompts depends heavily on the clarity of the input requirements and the capabilities of the host LLM executing this meta-prompt. Performance may vary across different LLMs.
